---
title: "A Nuging Experience - Part 1"
date: 2025-05-09 15:23:11
categories:
  - Nudging and Manipulation
author_staff_member: maheep-gupta
image: /images/button.png
large_header: false
---

## When Nudging Becomes Manipulation
Have you ever opened an app “just for a minute,” only to look up an hour later wondering where the time went? That’s not just chance, it’s design.

Today’s most popular platforms like TikTok, Instagram, and Netflix are built to capture and keep your attention. They use subtle interface choices, what designers call nudges or choice architecture, to guide your behavior. These nudges might seem harmless, even helpful. But when they tap into your habits, emotions, and psychological biases to push you toward endless scrolling or binge-watching, the line between persuasion and manipulation starts to blur.

This article explores how modern digital products are crafted not just to serve users, but to shape their behavior, often in ways they don’t fully notice. Through real-world examples, we’ll look at how platforms use infinite feeds, social feedback loops, autoplay, and personalization to keep us hooked. Along the way, we’ll ask: at what point does “good design” cross into something more troubling?

Understanding these techniques is the first step in regaining control over your screen time and making tech work for you, not the other way around.

### Nudging, Choice Architecture, and Manipulation in UX

Nudging refers to influencing choices through the way options are presented (the “choice architecture”) without removing freedom of choice. In UX, this could mean setting beneficial defaults or subtly guiding user behavior. Thaler and Sunstein’s classic definition describes a nudge as any aspect of choice architecture that alters behavior predictably without forbidding options or significantly changing incentives. For example, an app might default to moderate notification settings to encourage healthy use – a well-intentioned nudge. In contrast, manipulation implies steering users toward a behavior that may not be in their best interest, often by exploiting cognitive biases or hidden influences. A manipulated choice is one where the user’s autonomy or informed judgment is substantially undermined. As one legal definition puts it, a manipulative “dark pattern” UI is designed with the effect of subverting or impairing user autonomy. The line can be blurry – all nudges exert influence, but we tend to view them as manipulative when they are deceptive, covert, or self-serving against the user’s welfare.

In the context of social media and content apps, companies engage in “digital choice architecture”: designing interfaces (feeds, buttons, prompts, defaults) that shape how users behave online. The intent is often to maximize engagement. When these designs simply make an app more intuitive or personalize content to user tastes, they may be seen as benign persuasion or customer-focused nudging. However, when they leverage psychological weaknesses to lock in users’ attention (for example, endless scrolls or delayed rewards that exploit our reward circuitry), critics argue they cross into manipulation. Notably, researchers have pointed out that the difference lies in outcomes: persuasion respects the user’s agency, whereas manipulation often leads users to act against their own initial intentions or well-being. In short, choice architecture becomes ethically problematic when it nudges users toward harmful behaviors (e.g. excessive screen time, addictive usage) rather than their own goals. The sections below illustrate this tension via case studies of major platforms known for their engaging – and at times habit-forming – UX designs.

### Case Study: TikTok – Infinite Feed and Intermittent Rewards

TikTok’s meteoric rise is often attributed to an extremely engaging user experience engineered through powerful behavioral techniques. Central to TikTok is the “For You” infinite feed, a never-ending scroll of short videos tailored to the user. This design deliberately removes stopping cues – there is always another video cued up, making disengagement difficult by default. The feed is powered by a highly adaptive recommendation algorithm that learns a user’s preferences with each swipe. In essence, TikTok runs a continuous experimentation system akin to a contextual bandit, dynamically picking content that maximizes the reward (user watch time) for that specific user. As the user reacts (watching, replaying, skipping), the algorithm refines its suggestions in real time. The result is a personalized content stream that becomes more and more irresistible the longer one uses the app.

A defining element of TikTok’s design is the use of intermittent reinforcement – delivering rewards on an unpredictable schedule. Not every video is a hit, but the next swipe might surface something delightful or exciting, which creates a compulsion to keep swiping. Psychologically, this mimics a variable reward schedule, known to be most effective for conditioning behavior (similar to how slot machines hook gamblers). Studies have noted the striking parallel: “the variable pattern of reward on TikTok simulates the intermittent reward pattern of a slot machine; this keeps individuals engaged under the impression that the next play might be ‘the one.’”. Users get sucked into what Natasha Schüll (an addiction researcher) calls “ludic loops” – repeated cycles of uncertainty (what will the next video be?), anticipation, and feedback, with just enough reward to keep you going. Over time, these unpredictable dopamine hits forge strong habit loops. Indeed, TikTok’s likes, comments, and visual stimuli act as rewards that reinforce the behavior of continual scrolling. Neuroscientific research on similar social apps shows that receiving “likes” activates reward centers (e.g. the nucleus accumbens), releasing dopamine and encouraging repetition of the behavior. TikTok doubles down on this by making each viewing session a fast-paced reward cycle – short videos that quickly deliver a punchline or cool trick before prompting the user with another enticing thumbnail.

Critically, TikTok’s choice architecture makes disengagement non-intuitive. The default is to keep watching: videos autoplay one after another; to stop, the user must exert a conscious override (there’s no natural end-point as with a TV episode or a webpage). The app’s design minimizes cognitive load – swipe down and new content appears effortlessly – keeping the user in a fluid “flow” state that obscures the passage of time. Many users report losing hours on the app without realizing it. In UX terms, TikTok has removed friction for continued use, but added friction to stopping (one has to willfully close the app, something the immersive design makes easy to postpone). The ethical rub is that these nudges primarily serve TikTok’s interest (maximizing ad impressions and user metrics) while potentially harming users through overuse. As one analysis bluntly put it, TikTok’s success “depends on its ability to manipulate users to continue use despite any adverse consequences”. This highlights how a series of small, ostensibly user-friendly nudges (personalized suggestions, autoplaying content, infinite scroll) can aggregate into a manipulative system that exploits our psychological susceptibility to instant rewards and social validation. TikTok’s case exemplifies the overlap of advanced AI-driven personalization (contextual bandit algorithms) with age-old behavioral conditioning techniques to drive habit formation on a massive scale.

### Case Study: Instagram – Social Feedback Loops and FOMO

Instagram employs its own arsenal of behavioral design strategies to keep users glued to the platform. At its core, Instagram’s UX leverages social rewards and fear of missing out (FOMO) as powerful motivators for engagement. The ubiquitous “Like” button is a prime example of a seemingly simple feature that profoundly influences user behavior. Each like on a user’s post is a form of positive reinforcement – a quantifiable social approval that triggers a dopamine boost and encourages the user to keep posting and checking in. Likewise, giving likes to others provides a smaller reward (a sense of social contribution or anticipation of reciprocity). This mutual reinforcement system creates a habit loop: users return frequently to see new likes or comments on their content, or to scroll for new posts to like, in turn seeking social validation for themselves. Over time, the cycle of posting and feedback can lead to habitual or even compulsive app use, as users grow conditioned to seek that next hit of social affirmation. In effect, Instagram nudges users to invest in the platform (by curating an attractive profile, garnering followers, etc.), and this investment increases their commitment to returning.

Beyond likes, Instagram’s infinite scroll design and pull-to-refresh mechanism on the home feed echo the slot-machine-style engagement we discussed with TikTok. Each pull-to-refresh is like pulling a lever: sometimes you see something exciting at the top of your feed (a new post from a close friend, a fascinating update), other times nothing of note – the variability itself keeps you checking. “You pull a lever and immediately receive either an enticing reward... or nothing… We cannot know when we will be rewarded... But that’s precisely what keeps us coming back.” explained former Google design ethicist Tristan Harris, drawing the parallel to gambling machines. Instagram was one of the early adopters of infinite scrolling in social apps, and this interaction pattern (invented in 2006 by Aza Raskin) was explicitly designed for compulsion, not convenience – it exploits our propensity to keep searching for rewarding content endlessly. By continuously loading new posts and updates, Instagram’s feed provides no natural breakpoint at which a user might easily decide to close the app, thereby nudging them to spend more time than they planned.

Another stickiness factor on Instagram is the Stories feature. Instagram Stories (and similar features like Snapchat’s or Facebook’s) are photos/videos that disappear after 24 hours. This ephemerality taps into FOMO – users feel pressure to check the app frequently so as not to miss friends’ fleeting updates. The design trick here is the limited-time availability: by making content temporary, Instagram nudges users to engage now (immediate gratification) rather than delaying (which, thanks to hyperbolic discounting, many will do anyway). The result is users opening Instagram multiple times a day in short bursts to “keep up,” which cumulatively increases screen time. Additionally, Instagram sends push notifications by default – for example, alerts that “your friend just posted for the first time in a while” or subtle nudges like “see who liked your photo.” These default notifications serve as external triggers (in Nir Eyal’s Hook Model terms: triggers → action → reward → investment) that cue users to return, often pulling them back in at moments when they weren’t actively intending to use the app. By populating the notification feed with social cues, Instagram reduces the cognitive effort for the user to re-engage – they are continuously invited back with enticing information.

From a choice architecture perspective, Instagram defaults to engagement. It opts users into data feeds and notifications that maximize time-on-app (though users can dig into settings to limit these, few do). Disengaging – whether it’s turning off all those notifications, or simply resisting the urge to scroll – is made difficult by the constant stream of social feedback and fresh content. The cognitive load to process Instagram’s interface is kept low (simple swipe and tap gestures), meaning users can easily fall into mindless browsing without heavy mental effort. This is by design: UX best practices often say to minimize friction. However, in the context of persuasive design, frictionless = effortless continuation of the behavior. Some experts now advocate “design friction” in such apps – intentional pauses or check-ins (for instance, Instagram’s “You’re All Caught Up” message after scrolling past recent posts) – to nudge users away from overuse. Instagram did introduce that message to signal when you’ve seen everything new, an attempt to balance the infinite nature of its feed. Yet the app quickly fills the gap below with suggested content, so the scroll truly never has to end. Thus, Instagram illustrates the nuanced interplay of nudges: it uses social and design nudges to deeply engage users, but also faces pressure to nudge in the opposite direction for wellbeing – a tension at the heart of the nudging vs. manipulation debate.

### Case Study: Netflix – Auto-Play and Personalized Binge-Watching

Unlike social media, Netflix’s goal isn’t user-generated content or social interaction, but rather prolonged content consumption. The platform famously optimized its UX to encourage “binge-watching” of shows and movies. A cornerstone of this strategy is the “auto-play” feature: when an episode ends, Netflix automatically queues and begins the next one after a short countdown. This design is a deliberate piece of choice architecture – doing nothing (default user inaction) results in continuing to watch, whereas opting to stop requires the user to actively intervene (pressing “back” or closing the app). By making continuous viewing the path of least resistance, Netflix heavily nudges users to keep going. According to a former Netflix product developer, the company rigorously A/B tested such features against key engagement metrics (“hours watched” and retention). The auto-play feature (“Post-Play”) yielded “by far the biggest increase in hours watched … of any feature we ever tested.” In fact, Netflix experimented with the timing of the countdown and found that 10 seconds was initially optimal to hook viewers into the next episode (providing just enough time to internalize the last episode’s events but not enough to reconsider starting another). After viewers became accustomed to bingeing, Netflix later shortened the gap to 5 seconds, indicating that “Netflix users have become conditioned to expect autoplay,” and a faster roll into the next show further increased watch time. This is a clear example of reinforcement learning in UX – users were trained to continue watching, and as their behavior adapted, Netflix adjusted the nudge (a shorter delay) to strengthen the habit.

Netflix also employs dynamic personalization algorithms to present content most likely to keep each user watching. From personalized thumbnails (tailored images for the same movie shown to different users based on their viewing history) to rows of recommendations (“Because you watched X”), Netflix’s interface is unique to each profile. These recommendation engines often use techniques related to contextual bandits or multi-armed bandits: effectively, Netflix is constantly choosing which titles or thumbnails to show you, learning from your clicks and views to improve the relevance of future suggestions. The goal is to maximize the reward (in this case, the user selecting another title to watch) by learning your preferences. Over time, this algorithmic curation nudges users into “rabbit holes” of content – for example, if you watch a lot of thriller movies, your Netflix homepage becomes dominated by thrillers, increasing the odds you’ll find another immediately appealing option rather than signing off. While personalization enhances user satisfaction up to a point, it also serves Netflix’s interest by always surfacing something engaging now, catering to our present bias (why log off and be bored, when another tailored entertainment is one click away?). Hyperbolic discounting plays a role: viewers may intend to only watch one episode (knowing they should sleep early, a future benefit), but when faced with an immediate reward of entertainment (next episode starting in 5 seconds), the short-term temptation often wins. Netflix’s UX is crafted to capitalize on that bias, delivering instant gratification with minimal friction (even skipping the intro of episodes by default to get you hooked faster).

It’s worth noting Netflix’s approach to nudging is not about social feedback like others, but about content cadence and convenience. The service even experimented with a “Next Episode” prompt back in the DVD era (where users had to manually play the next episode) and found that reducing user effort increased consumption. By the streaming era, auto-play became a standard. The platform does introduce a gentle check after several episodes – the “Are you still watching?” prompt – which is arguably a nod to user welfare (or simply to stop wasting bandwidth if you fell asleep). However, this comes only after a considerable binge session. Internally, Netflix’s framing was unapologetic: “Netflix wants you to spend more hours watching… and is scientifically engineering the product to make it more addictive,” said the former product engineer. This candor underscores that what might be euphemistically called “engagement optimizations” are, in practice, carefully tested nudges that push users toward behavior that benefits the platform (more viewing) even if it undermines the user’s initial self-regulation goals. Netflix balances this by pointing to user choice (the ability to turn off autoplay, set profile PINs, etc.), but these are not the default. The default design makes disengagement a decision point the user must actively confront – and psychology tells us many will go with the flow and continue passively. Netflix’s success with auto-play influenced the whole industry; YouTube, Amazon Prime Video, Facebook, and others all implemented similar continuous play or feed mechanisms once Netflix demonstrated how effective it was in driving screen time.

### The Blur Between a Nudge and a Manipulation

The case studies above illustrate how UX design can nudge users into prolonged engagement, often leveraging subconscious tendencies. But when does a nudge become a manipulation? One clear indicator is when the design consistently drives behavior that users themselves later perceive as against their interests (for instance, “I wish I hadn’t spent two hours scrolling, but the app made it hard to stop”). If the default options and interface make it easier to stay hooked than to quit, users’ autonomy may be undermined. All the platforms discussed use defaults favoring more engagement: infinite feeds instead of finite content, auto-play instead of stopping, opt-out rather than opt-in notifications. These defaults intentionally raise the friction of disengagement. Users must muster extra cognitive effort to override the app’s flow – e.g. hitting pause on Netflix or resisting the pull-to-refresh on Instagram – which in moments of low willpower or high stress, they may not do. This exploit of our tendency to take the path of least resistance can cross into manipulation when it leads to habitual use people struggle to control. Indeed, experts liken these app experiences to addictive behaviors. Research in behavioral addiction notes that the unpredictable rewards and constant availability in apps can create dependency similar to gambling or substance addiction, activating the same brain reward mechanisms. Users might experience cravings, anxiety when unable to check, and even phantom notifications (feeling the phone buzz when it hasn’t) – signs that the product has ingrained itself deeply. Design elements like Snapchat’s “streaks” (which reward friends for messaging each other daily and make breaking the streak painful) are a blatant example of a nudge verging on coercion; teens admit they maintain streaks even when they don’t feel like chatting, just to avoid the negative consequence. Such designs effectively create obligations for the user that only serve the platform’s engagement metrics.

Another hallmark of manipulation is opaqueness. In many of these apps, users are not fully aware of how the algorithmic curation or notifications are steering them. For instance, Facebook’s news feed in the 2010s was tuned to show emotionally charged content (outrage-inducing posts, sensational news) because as the saying went, “if it’s enraging, it’s engaging”. The user just sees what looks like a neutral feed, but in reality the system is promoting content that will glue their eyes to the screen (potentially at the cost of showing more balanced or calming content). This crosses into what the EU’s draft AI regulation calls “subliminal techniques” – influences below the level of conscious awareness that significantly alter behavior. An AI-driven platform might, for example, learn that a certain user scrolls longer when shown posts that trigger anxiety or insecurity, and start serving more of that content. The user may not realize this manipulation; they simply feel compelled to stay online. Such practices blur consent and exploit psychological vulnerabilities, raising ethical red flags. Persuasion becomes manipulation when it “coerces individuals into making decisions they wouldn’t otherwise consider” by using their emotional triggers against them. Importantly, the intention behind design choices matters: deliberately engineering features to cause addictive use (as opposed to incidental side effects of a useful feature) leans toward manipulation. In practice, it can be hard to discern intention, but internal documents and whistleblowers (like the Netflix engineer’s comments, or the Facebook Papers leaks) often reveal engagement-at-all-costs mindsets.

From an ethical standpoint, some scholars argue that nudging itself is not inherently wrong – what matters is the outcome toward which users are nudged. If it’s towards something that undermines their well-being (e.g. an “unhealthy amount of time” on an app or harmful content), then even an otherwise permissible nudge becomes problematic. In social media, unfortunately, the incentives often align nudges with outcomes that are at least suspect. Platforms profit from maximal attention, while users suffer opportunity costs (time wasted, reduced attention spans, mental health effects). The manipulation is evident when companies persist in these design patterns despite knowing the potential harm. For example, even after public outcry about infinite scroll’s addictiveness, most apps did not remove it; the inventor of infinite scroll Aza Raskin lamented that his invention was wasting hundreds of thousands of human lifetimes per day, apologizing for the unintended negative impact. Yet, the industry as a whole stuck with the design because it is so effective at engagement. This underscores that what might start as a clever UX improvement can become a dark pattern if the company prioritizes engagement over users’ agency. Features like “dark mode” or “nightly reminders to log off” could be nudges in the user’s interest (reducing eye strain or prompting sleep), but these are less often pushed, as they conflict with the engagement goal. In summary, the difference between a nudge and a manipulation in app design often comes down to whether the design respects the user’s long-term, reflective choices or exploits their short-term, impulsive reactions. The more an interface relies on the latter – hooking users through instant gratification while making it hard to choose otherwise – the more it veers into manipulation.

A user experiencing fatigue or stress while on their phone. Many design patterns that maximize screen time (like endless feeds and auto-play) can lead to “app fatigue,” where users feel drained or frustrated but still have difficulty disconnecting. Such negative effects highlight when persuasive design might have crossed into manipulation.

### Conclusion

Modern apps aren’t just designed to be helpful — they’re optimized to hold your attention. Through clever nudges, habit-forming loops, and frictionless interfaces, platforms like TikTok, Instagram, and Netflix guide your behavior in ways you may not even notice. While some of these features improve user experience, others quietly chip away at your autonomy, steering you toward more screen time, more clicks, and more data shared.

The key is awareness. Understanding how these platforms work, and where design crosses into manipulation, is the first step in reclaiming control. Ask yourself: Is this app helping me meet my goals, or just keeping me hooked?
Design doesn’t have to be exploitative. With enough public pressure and digital literacy, we can push for tech that respects our time, our choices, and our well-being.

In our next article, we’ll explore how laws and regulators are responding to these manipulative patterns and what ethical design could look like in the future. Lastly, we will finish this 2-part series and draw a more general conclusion. 

#### References

References for the text above can be found in the second part of the series!
