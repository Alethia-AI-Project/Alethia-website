---
title: "A Nuging Experience - Part 2"
date: 2025-05-23 09:47:01
categories:
  - Nudging and Manipulation
author_staff_member: maheep-gupta
image: /images/button.png
large_header: false
---
## Beyond the Scroll – What Really Keeps Us Hooked?

In our [previous article](#), we explored how platforms like TikTok, Instagram, and Netflix use design patterns to guide user behavior in subtle but powerful ways. These choices, often framed as “nudges,” can slide into manipulation when they exploit psychological shortcuts or undermine our autonomy.

But how exactly do these persuasive systems work and why are they so hard to resist?

In this follow-up, we take a closer look at the psychological principles behind habit-forming apps and how design taps into our brain’s reward systems. We also explore the growing legal and regulatory efforts to define and limit manipulative digital practices. The goal: to better understand what’s happening beneath the surface of our screens and what can be done about it.


## Psychological Principles Behind Habit-Forming Apps

Several key psychological and behavioral economics concepts help explain why these techniques work so well — and how they sometimes exploit our minds’ vulnerabilities:

### Intermittent Reinforcement & Dopamine Loops

Variable rewards (intermittent reinforcement) are fundamental. B.F. Skinner’s research showed that subjects (whether pigeons or people) become most addicted to behaviors that pay off unpredictably. Social apps and content feeds are built on this principle – you don’t know if the next scroll will bring something amazing or mediocre, which creates a compulsive “just one more” mindset. Each reward (a funny video, a like notification, an interesting comment) causes a release of dopamine, reinforcing the action that preceded it (scrolling, checking the app). Over time, through classical conditioning, even the cue or context (hearing a notification ping, or seeing the app icon on your screen) can trigger a craving to check, because the brain has been trained to anticipate a reward. These dopamine loops are self-strengthening: the more you engage, the more you reinforce the neural pathways that prompt further engagement. This is how a casual app use can turn into a daily habit or even addictive behavior. Importantly, reinforcement learning is happening on two levels: the user is learning to use the app habitually (driven by rewards), and the app’s AI is simultaneously learning the user – tuning the rewards to be even more enticing. This dual-loop of mutual reinforcement (user behavior trains the algorithm, algorithm behavior trains the user) is a new dynamic unique to AI-driven digital platforms. It creates a feedback loop that can entrench usage patterns quickly and deeply.

### Habit Formation & the Hook Model

Persuasive technologists often deliberately design for habit formation. Nir Eyal’s **Hook Model** outlines a four-step loop:
- **Trigger** → **Action** → **Variable Reward** → **Investment**
Many apps follow this formula. Triggers can be external (a notification, an email reminder) or internal (boredom, FOMO, anxiety). The action is the simple behavior the app wants (open the app, scroll the feed). Then comes the variable reward – as covered, something satisfying but unpredictable that makes the user feel good (social praise, entertainment, useful info). Finally, investment is when the user puts something into the product that increases their likelihood of returning (e.g. creating content, customizing their profile, adding friends, or even as minor as “liking” posts which tailors their feed). This investment deepens the attachment because it gives the user a reason to come back (to see responses, or because the service is now personalized). Repeating this hook cycle builds a habit, meaning the user will start to use the app with less conscious thought, often triggered by internal cues. Over time, simply feeling a tinge of boredom might internally trigger someone to instinctively tap Instagram or YouTube – no external prompt needed. Platforms strive for this level of integration into users’ lives, essentially becoming the go-to solution for various emotional cues (loneliness triggers checking Facebook, curiosity triggers browsing TikTok, etc.). Habit formation itself isn’t nefarious – it can be great if the habit is exercising with a fitness app – but for attention-based apps, the habits formed tend to be time-consuming routines. And once a habit is formed, it’s difficult for users to break, even if they intellectually realize it may not be healthy, because by design it bypasses conscious self-regulation.

### Cognitive Load and Friction

Good UX minimizes unnecessary cognitive load, streamlining user actions. Paradoxically, this UX virtue can become a vice in the context of limiting usage. When an app makes it effortless to keep consuming content (swipe, scroll, or let autoplay do its thing) and difficult to find settings that might help moderate use, it takes advantage of our mental laziness or inattention. High cognitive load (complex steps) discourages actions. Platforms often apply this asymmetrically: making it low load to continue engagement, but high load to, say, adjust your recommendation preferences or disable addictive features. For instance, turning off YouTube’s autoplay requires going into settings and flipping a toggle – a one-time fix, but many users never bother, especially if they’re not even fully aware that autoplay is a setting rather than a hardwired feature. Similarly, social media apps often require several obscure menu taps to disable notifications or set time limits. This is reminiscent of dark patterns that increase friction for choices the company doesn’t want (like cancelling a subscription). On the flip side, some researchers suggest introducing “positive friction” to combat overuse – features like a pop-up that asks “Do you want to continue watching?” or default daily time reminders. Such friction gives the user a moment of System 2 (deliberative thinking) to potentially break out of the automatic loop. But implementing these can conflict with business goals, so we often only see them after public pressure (e.g. when Apple and Google added Screen Time and Digital Wellbeing tools to their operating systems as a response to user concerns, essentially admitting that some external friction is needed to help users manage rampant app design). The interplay of cognitive load in design shows that what’s good for short-term user experience (seamless flow) may be bad for long-term user autonomy. Designers are increasingly aware of this trade-off.

### Hyperbolic Discounting & Instant Gratification

Human decision-making tends to overvalue immediate rewards and undervalue future outcomes – a bias called hyperbolic discounting. Apps exploit this by consistently offering an appealing immediate reward: watch another funny video now (versus the “cost” of missing out on some laughs if you log off), or gain social attention now (versus the abstract benefit of getting more sleep). Because users discount the future, they often go for the instant payoff – and the apps ensure that payoff is right at their fingertips at all times. This contributes to phenomena like procrastination via apps. Why do that hard work due tomorrow when you can get a quick dopamine hit scrolling today? In theory, users “know” this trade-off might hurt them later, but the app’s design keeps the focus on the now – with auto-playing content, constant refresh, etc., the future consequences are out of sight, out of mind. Hyperbolic discounting also interacts with notification design. Many apps will nag you with “friend X mentioned you” or “you have 5 unread messages” – nudges that something needs your attention now. The fear of missing something urgent or juicy makes you prioritize checking the app now, even if it interrupts more important tasks. This immediacy is amplified by design choices like bright red notification badges (red triggers urgency) and numbers that spur our completionist drive to clear them. In summary, app UX often frames choices such that the immediate reward is salient and one click away, while the user’s long-term goals (like not getting distracted) are not represented in the interface. Thus our natural present-bias leads to behavior that in aggregate might be against our long-term interest – a subtle form of self-manipulation encouraged by the product design.

### Social Norms and Peer Pressure

A psychological aspect not yet highlighted is how design leverages social influence. Features like visible follower counts, read receipts (“seen” indicators), and public like counts (though Instagram has experimented with hiding like counts to reduce pressure) all serve to entangle our social self in the app. For instance, if everyone in your circle is actively posting and responding, you may feel obliged to do the same to belong. WhatsApp’s blue ticks showing you read a message nudge you to respond quickly (stoking anxiety if you don’t). Group dynamics and peer effects mean these design elements nudge collective behavior, not just individual habits. While not manipulation per se, they can heighten the difficulty of disengaging – leaving a platform might mean social exclusion or missing community updates. This social architecture is part of the “stickiness”.


In combining these principles, today’s apps create powerful behavioral change engines. An illustrative summary is that they use triggers (often via notifications or internal urges), simplify the action (frictionless UI), deliver variable rewards (content and social feedback), and encourage investment (personalization, network building), all while our cognitive biases (like present bias and FOMO) tilt us toward staying engaged. The net effect is an experience that feels user-driven (“I’m choosing to scroll”), but is heavily orchestrated by designers and algorithms aligning with known psychological levers. Recognizing these constructs is the first step for users to reclaim some control – hence the rise of digital literacy efforts about things like dopamine-driven design and how to mitigate it (e.g., manually turning off autoplay, disabling notifications, using grayscale mode to make apps less attractive, etc.). From an industry view, it raises the question: should designers exercise restraint or counter-nudges to avoid exploitative outcomes? This is where legal and regulatory considerations come into play.

## Legal Considerations: Regulating Manipulative Design and AI

The law traditionally has addressed deception and coercion, but manipulation by UX or AI is a relatively new challenge. Globally, regulators are starting to pay attention to dark patterns and manipulative interfaces. For example, in the United States, the Federal Trade Commission (FTC) has declared a crackdown on “manipulative dark patterns” that trick or trap consumers. Under existing laws (like Section 5 of the FTC Act against unfair or deceptive practices), cases have been brought against companies for things like making cancellation hard or sneaking in charges – essentially penalizing certain extreme choice architecture tricks. Several U.S. states’ privacy laws (CCPA/CPRA in California) also explicitly define and ban dark patterns that subvert user intent in areas like consent. As cited earlier, California’s CPRA defines a dark pattern as a UI that “has the substantial effect of subverting or impairing user autonomy”. While that was meant for privacy consent interfaces, the concept applies to engagement tactics as well. This gives regulators a broad mandate to judge when an interface crosses the line. However, proving manipulation is tricky – one must typically show that a design practice was intended to and had the effect of significantly diminishing users’ ability to make free, informed decisions. Unlike fraud, which has clear lies, manipulation can be more subjective (e.g. was an infinite scroll “unfair”? A company could argue the user always had the choice to stop). The lack of a bright line makes enforcement challenging. Nonetheless, agencies are gathering evidence – for instance, the FTC’s 2022 report documented how seemingly innocuous design features can mislead or pressure users, signaling that companies could face penalties if their engagement tactics cause substantial consumer harm.

The European Union is on the forefront of attempting to regulate manipulative designs, especially those powered by AI. The proposed EU Artificial Intelligence Act (AI Act) (expected to be one of the first comprehensive AI laws) includes provisions that ban certain manipulative AI practices outright. In its Article 5, the AI Act prohibits systems that deploy “subliminal techniques beyond a person’s consciousness in order to materially distort their behavior in a manner that causes or is likely to cause harm,” as well as AI that “exploits vulnerabilities of a specific group (like children or persons with disabilities) to materially distort behavior resulting in harm.” These clauses directly target AI-enabled manipulation. For example, an AI that unbeknownst to users flashes imperceptible cues or plays inaudible sounds to influence people (a hypothetical extreme case) would be banned. A more everyday example given by EU officials was an AI toy that encourages kids to perform dangerous acts – clearly manipulative and harmful. But how would this apply to social media algorithms that manipulate engagement? It sets an interesting precedent: if an algorithm is proven to intentionally manipulate users in a harmful way, it could be illegal in the EU. The difficulty is the wording: “beyond a person’s consciousness” and requiring likely harm. Recommender algorithms like TikTok’s operate at the edge of consciousness – they don’t use subliminal stimuli, but they do leverage psychological biases covertly. And the harm is debated: does loss of time or addiction count as “psychological harm” under the law? It might, but it’s not as clear-cut as physical harm. Moreover, intent is key. The EU law (in Recital 16) emphasizes that intent to manipulate must be present – it’s not enough that an AI system happened to influence someone; it had to be built or used for that manipulative purpose. This is a high bar and a potential loophole. Companies can claim their aim was engagement or user preference satisfaction, not manipulation, even if the effect is the same. Since much of the manipulative aspect comes from exploiting general human biases rather than individualized “vulnerabilities” (except in the case of minors, which are explicitly protected), platforms might not fall under these bans unless regulators interpret psychological addiction as a harm and aggressive engagement optimization as an intent to “materially distort behavior.”

Separately, the EU’s Digital Services Act (DSA) and Digital Markets Act (DMA) include provisions against dark patterns and require greater transparency in online platforms’ interfaces. The DSA, for instance, mandates that cancelling a subscription should be as easy as signing up (to avoid manipulative retention tricks). While these are more focused on e-commerce and consumer rights, they reflect a broader regulatory recognition that UI design can impinge on user autonomy and therefore must be fair. For AI-driven content feeds, transparency requirements (users should be informed that what they see is personalized by an algorithm, with options to adjust it) are being introduced. In theory, telling users “this feed is ranked to maximize your engagement” could make them more cautious – though in practice, transparency alone often doesn’t change behavior.

In the developed world, we also see discussions around treating excessive engagement as a consumer protection or even public health issue. For example, lawmakers have contemplated whether addictive social media features should be regulated like addictive substances. Thus far, no major jurisdiction has outright limited how engaging an app can be (there’s no “maximum scroll time” rule – that would be impractical). Instead, the focus is on process: ensuring users aren’t deceived or unfairly coerced, and that vulnerable populations (like children) are not targeted with designs that can harm them. The UK’s Age-Appropriate Design Code (a code of practice under its data protection law) requires online services likely to be accessed by children to consider the best interests of the child – which has led some platforms to disable auto-play by default for teen accounts, limit notifications at night for younger users, etc., as a precaution. This indicates a regulatory leaning that what might be acceptable for adults (who can presumably self-regulate, though that’s debatable) is not okay for minors. Indeed, if we think of manipulation in terms of exploiting a lack of impulse control or cognitive maturity, children are the clearest case where persuasive design becomes unethical. Lawsuits in the U.S. are now emerging, accusing social media companies of knowingly designing addictive products that hook teenagers, thus causing harm – essentially applying product liability concepts to UX design. The outcome of these will be important in setting precedents.

Another angle is AI transparency and oversight. The EU AI Act will require high-risk AI (not sure if recommender systems will be classed as high-risk) to undergo assessments and possibly disclose when content is AI-selected. The hope is that if users understand an algorithm is proactively choosing what they see in order to maximize certain metrics, they might be more skeptical of the “nudges” embedded in that (like outrage-heavy feeds). However, given cognitive biases, just knowing about the manipulation doesn’t fully remove its power. For example, even if you know infinite scroll is designed to keep you hooked, you might still fall prey to it when tired at midnight.

In summary, legal frameworks are beginning to catch up to the idea that manipulation via design and AI is a real risk to consumers. The EU is leading with explicit bans on the worst forms of AI-fueled manipulation and a broader push for transparency and fairness in online choice architecture. The U.S., through FTC enforcement and state laws, is tackling dark patterns on a case-by-case basis. The biggest challenge in regulation is definition and proof: one must define what level of influence is unacceptable and then demonstrate that a company’s design crossed that line. Because so much of manipulative design leverages voluntary behaviors (just nudged in a certain way), companies have plausible deniability – “we’re just giving users what they enjoy, they can quit anytime.” The counterargument from researchers is that these systems can hijack decision-making in a way users alone can’t be expected to overcome, thus necessitating oversight. It’s a delicate balance: we don’t want to forbid persuasive design wholesale (which is ubiquitous and often beneficial), but we also don’t want AI supercharging harmful manipulation at scale. Expect to see continued debate, academic research, and refined laws in the coming years as society decides where to draw that line in the UX of the digital attention economy.

## Comparison of Behavioral Design Strategies Across Platforms

| Platform             | Behavioral Design Strategies                                                                                                  | Examples & Effects |
|----------------------|-------------------------------------------------------------------------------------------------------------------------------|---------------------|
| **TikTok**           | - Infinite scroll feed with autoplay  <br>- Intermittent variable rewards (unpredictable video quality) <br>- Contextual bandit algorithm rapidly personalizing content <br>- Minimal UI friction (swipe for next video) | TikTok’s “For You” feed acts like a slot machine — unpredictable, fast-paced, and rewarding just enough to keep you swiping. The personalized algorithm adapts in real time, reinforcing habit loops and making time feel distorted. Users often binge-watch without noticing, creating compulsive use patterns. |
| **Instagram**        | - Infinite scroll and pull-to-refresh  <br>- Social rewards (likes, comments) and social pressure (visibility of engagement) <br>- Ephemeral Stories inducing FOMO <br>- Personalized Explore recommendations | Instagram uses variable rewards and social validation to create a loop of posting and checking. Stories and notifications increase FOMO and encourage multiple check-ins a day. The interface is frictionless, and once scrolled past new posts, suggested content keeps the feed going indefinitely. |
| **YouTube / Facebook** | - Autoplay next video by default  <br>- Endless recommendations (“Up Next”, algorithmic feeds) <br>- Emotionally charged content to boost engagement <br>- Continuous scroll and default notifications | YouTube and Facebook auto-play content and promote emotionally arousing or sensational material to maximize time on site. Recommendations become increasingly personalized (and sometimes extreme), pushing users down content rabbit holes. Friction to stop watching is low; scrolling continues passively. |
| **Netflix**          | - Auto-play next episode in series  <br>- Countdown timer pressure (e.g., “Next episode in 5…4…”) <br>- Personalized thumbnails and home screen layout <br>- Extensive A/B testing for maximum retention | Netflix’s Post-Play feature reduces friction to binge-watch. The countdown to the next episode is optimized to bypass user resistance. Personalized visuals and ordering of content keep users engaged. Even intro skips are defaulted to save time and prolong viewing. Netflix’s system encourages long viewing sessions framed as user choice. |


## Conclusion

Modern app and platform design operates on a continuum from gentle nudging to outright manipulation. By carefully architecting choices and defaults – from infinite feeds and auto-play to strategically-timed rewards – companies like TikTok, Instagram, and Netflix have learned to hack our attention. They apply insights from psychology (variable reinforcement, habit loops, cognitive biases) and AI (contextual bandits and personalization) to create experiences that users find incredibly engaging, often more than they bargain for. This has yielded unprecedented screen time and commercial success, but also growing concern that users are being manipulated into behaviors that may harm their well-being (excessive usage, addiction-like symptoms, distorted decision-making). The ethical tension is palpable: is it acceptable to leverage subconscious influences if the user can technically opt out, or does there come a point where design overrides autonomy and thus warrants intervention?

Industry case studies show many common patterns – what one might call a “playbook” of behavioral design for engagement – and these have raised flags among psychologists, ethicists, and now lawmakers. The developed world is beginning to respond: we see a push for transparency, option to opt-out of algorithmic feeds, and even outright bans on the most insidious manipulative practices (as in the EU’s pending regulations). Still, enforcement and practical boundaries remain difficult. It’s hard to legislate an app to be less “sticky” without clear metrics of harm. As such, much responsibility still lies with designers and companies to consider the long-term impacts of their choices. Some have started talking about “Time Well Spent” and user digital well-being, indicating a possible shift away from pure maximization of screen time toward a more user-centric definition of success.

For users, being aware of these mechanisms is crucial. Understanding that that urge to pull-to-refresh or watch one more episode isn’t purely your willpower failing – it’s the product of careful design – can empower more mindful usage and encourage demand for better defaults. In an ideal future, apps might use the same powerful techniques to nudge us towards healthier behaviors (as some wellness and educational apps do) rather than just deeper content pits. Until then, the overlap and tension between nudging and manipulation in UX will remain a subject of debate, requiring input from interdisciplinary research, public policy, and ethically minded design leadership. By shedding light on these practices through case studies and analysis, we take a step toward ensuring that technology serves our interests, instead of irresistibly steering us in ways we’ll regret.


### References

References for part 1 and part 2 of this 2-part series can be found below:

## References

1. **Campbell, D. (2022).** *Nudging and Social Media: The Choice Architecture of Online Life.*  
   Explores how social media companies nudge increased usage and argues that nudging per se is permissible — but nudging toward harmful outcomes is the issue.

2. **Busby, M. (2018).** *Social media copies gambling methods 'to create psychological cravings'.* The Guardian.  
   Reports on how social platforms mimic slot machines using variable rewards and “ludic loops” to drive compulsive behavior.

3. **Bhargava, H.K. (2023).** *If It’s Enraging, It is Engaging: Infinite Scrolling in Information Platforms.*  
   Academic paper detailing how platforms exploit cognitive biases like negativity bias and design addictive infinite scroll features.

4. **Brown University, Journal of Public Health (2021).** *What Makes TikTok so Addictive?*  
   Analyzes how TikTok leverages short videos, algorithmic feeds, and variable rewards — likened to slot machines — to foster flow states and compulsive use.

5. **Hacker News (2019).** Comment by user *rsweeney21*.  
   First-hand account from a Netflix engineer confirming autoplay was A/B tested to boost hours watched, calling it “scientifically engineered… to make it more addictive.”

6. **Holistic AI (2024).** *Navigating Prohibited Practices under the EU AI Act.*  
   Overview of AI Act provisions that ban subliminal or manipulative systems, including examples from social media algorithms and engagement-driven content.

7. **KU Leuven CiTiP Blog (2023).** *My brain hurts! Can the AI Act protect mental harm?*  
   Legal analysis of the AI Act’s manipulation clauses, focusing on the required intent and psychological impact of AI behavior distortion.

8. **Loeb & Loeb LLP (2022).** *If You Don’t Read This Article About Dark Patterns…*  
   Legal overview defining dark patterns, citing how user interface design can impair autonomy — especially in consent and engagement settings.

9. **Aly Juma (2023).** *Hyperbolic Discounting: Why We Prioritize Instant Gratification.*  
   Explains the human tendency to favor immediate rewards and how this is exploited in app UX design through notifications, auto-play, and endless content.

10. **Harris, T. (2016).** *“Never Get High on Your Own Supply”* (presentation).  
   Design ethicist Tristan Harris describes infinite scroll and pull-to-refresh as the digital era’s slot machines, powered by variable reward systems.

"""